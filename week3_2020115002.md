# Week 3, Lecture 1

## Product of two d-degree polynomials

The product of two degree-d polynomials is a polynomial of degree 2d.

Let A(x) = a<sub>0</sub> + a<sub>1</sub>x + · · · + a<sub>d</sub>x<sup>d</sup> and B(x) = b<sub>0</sub> + b<sub>1</sub>x + · · · + b<sub>d</sub>x<sup>d</sup>,
their product,

 C(x) = A(x) · B(x) = c<sub>0</sub> + c<sub>1</sub>x + · · · + c<sub>2d</sub>x<sup>2d</sup> ,
 has coefficients,
  c<sub>k</sub> = a<sub>0</sub>b<sub>k</sub> + a<sub>1</sub>b<sub>k-1</sub> + · · · + a<sub>k</sub>b<sub>0</sub> 

(for i > d, take a<sub>i</sub> and b<sub>i</sub> to be zero). Computing c<sub>k</sub> from this formula takes O(k) steps, and finding all 2d + 1 coefficients would therefore require O(d<sup>2</sup>) time.
But there exists a better solution!

### Alternative representation of polynomials:

A degree-d polynomial is uniquely characterized by its values at any d + 1 distinct points.
Fix any distinct points x<sub>0</sub>, . . . , x<sub>d</sub>. We can specify a degree-d polynomial A(x) = a<sub>0</sub> + a<sub>1</sub>x + · · · + a<sub>d</sub>x<sup>d</sup> by either one of the following:

1. Its coefficients a<sub>0</sub>, a<sub>1</sub>, . . . , a<sub>0</sub>
2. The values A(x<sub>0</sub>), A(x<sub>1</sub>), . . . , A(x<sub>0</sub>)

Since the product C(x) has degree 2d, it is completely determined by its value at any 2d + 1 points. And its value at any given point z is easy enough to figure out, just A(z) times B(z). Thus polynomial multiplication takes linear time in the value representation.
So we need to first translate from coefficients to values which is just a matter of evaluating the polynomial at the chosen points; then multiply in the value representation, and finally translate back to coefficients (interpolation)

![polynomial_representation](polynomial_representation.png)

#### Algorithm:
Input: Coefficients of two polynomials, A(x) and B(x), of degree d

Output: Their product C = A · B

Selection:
Pick some points x<sub>0</sub>, x<sub>1</sub>, . . . , x<sub>n-1</sub>, where n ≥ 2d + 1

Evaluation:
Compute A(x<sub>0</sub>), A(x<sub>1</sub>), . . . , A(x<sub>n-1</sub>) and B(x<sub>0</sub>), B(x<sub>1</sub>), . . . , B(x<sub>n-1</sub>)

Multiplication:
Compute C (x<sub>k</sub>) = A(x<sub>k</sub>)B(x<sub>k</sub>) for all k = 0, . . . , n − 1

Interpolation:
Recover C (x) = c<sub>0</sub> + c<sub>1</sub>x + · · · + c<sub>2d</sub>x<sup>2d</sup>


The selection step and the n multiplications take just linear time. But evaluating a
polynomial of degree d ≤ n at a single point takes O(n) steps. Hence, the total complexity is O(n<sup>2</sup>).

But there exists a better algorithm which does it in just O(nlog(n)) time, for a particularly clever choice of x<sub>0</sub>, . . . , x<sub>n-1</sub> in which the computations required by the individual points overlap with one another and can be shared.

### Fast Fourier Transform

We split A(x) into its odd and even powers,

A(x) = A<sub>e</sub>(x<sup>2</sup>) + xA<sub>o</sub>(x<sup>2</sup>),

where A<sub>e</sub>(·), with the even-numbered coefficients, and A<sub>o</sub>(·), with the odd-numbered coefficients, are polynomials of degree ≤ n/2 − 1.

Given paired points ±x<sub>i</sub>, the calculations needed for A(x<sub>i</sub>) can be recycled toward computing A(−x<sub>i</sub>):

A(x<sub>i</sub>) = A<sub>e</sub>((x<sub>i</sub>)<sup>2</sup>) + x<sub>i</sub>A<sub>o</sub>((x<sub>i</sub>)<sup>2</sup>)

A(-x<sub>i</sub>) = A<sub>e</sub>((x<sub>i</sub>)<sup>2</sup>) - x<sub>i</sub>A<sub>o</sub>((x<sub>i</sub>)<sup>2</sup>)

So, evaluating A(x) at n paired points ±x<sub>0</sub>, . . . , ±x<sub>n/2-1</sub> reduces to evaluating A<sub>e</sub>(x) and A<sub>o</sub>(x) (which each have half the degree of A(x)) at just n/2 points - (x<sub>o</sub>)<sup>2</sup>, . . . , (x<sub>n/2-1</sub>)<sup>2</sup>

![fft_evaluate](fft_evaluate.png)

The original problem of size n is recasted as two subproblems of size n/2, followed by some linear-time arithmetic. By recursion, we get a divide-and-conquer procedure:

T(n) = 2T(n/2) + O(n),

whose time complexity is O(nlog(n)).

But we have a problem: The plus-minus trick only works at the top level of the recursion. To recurse at the next level, we need the n/2 evaluation points (x<sub>o</sub>)<sup>2</sup>, . . . , (x<sub>n/2-1</sub>)<sup>2</sup> to be themselves plus-minus pairs. But a square cannot be negative. 
Hence, we need to use complex numbers.

We choose the complex nth roots of unity, that is, the n complex solutions to the equation z<sup>n</sup> = 1.
So we designed the FFT, a way to move from coefficients to values in time just
O(nlog(n)), when the points {x<sub>i</sub>} are complex n<sup>th</sup> roots of unity (1, ω, ω<sup>2</sup>, . . . , ω<sup>n-1</sup>)

{values} = FFT({coefficients}, ω)

FFT (polynomial formulation)
![fft_formulation_algo](fft_formulation_algo.png)

#### Interpolation:

It turns out that,

{coefficients} = (1/n)FFT({values}, ω<sup>−1</sup>)

Interpolation is thus solved using the same FFT algorithm, but called with ω<sup>−1</sup> in place of ω.

##### Proof:

![vandermonde_matrix](vandermonde_matrix.png)

Let the above matrix be M:
Evaluation is multiplication by M, while interpolation is multiplication by M<sup>−1</sup>

![vandermonde_omega](vandermonde_omega.png)

M<sub>n</sub>(ω)<sup>−1</sup> = (1/n)M<sub>n</sub>(ω<sup>−1</sup>) (Inversion formula)

#### FFT Algorithm
![fft_algo](fft_algo.png)

Finally, this divide-and-conquer strategy leads to FFT algorithm whose time complexity is O(nlog(n)).

