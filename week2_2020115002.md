# Week 2, Lecture 1

Whenever we have an algorithm, there are three questions we always ask about it:
1. Is it correct?
2. How much time does it take, as a function of n?
3. And can we do better?

## Fibonacci Series

The fibonacci series is a sequence of numbers each the sum of its two immediate predecessors.
0, 1, 1, 2, 3, 5, 8, 13, 21 .... so on.

Formally, the n <sup>th</sup> fibonacci number in the sequence (f <sub>n </sub>) is given by:
- F <sub>n </sub> = F<sub> n-1 </sub> + F <sub> n-2 </sub> if n > 1
- F <sub>n </sub> = 1               if n = 1 
- F <sub>n </sub> = 0               if n = 0 

### Algorithm 1 to calculate F <sub>n</sub>

This algorithm recursively calculates F <sub>n</sub>:

```python 
function fib1(n)
if n = 0: return 0
if n = 1: return 1
return fib1(n − 1) + fib1(n − 2)
```

#### Analysis:

For larger values of n, there are two recursive invocations of fib1, taking time
T(n − 1) and T(n − 2), respectively, plus three computer steps (checks on the value
of n and a final addition). Therfore, the recurrence relation comes out to be:

T(n) = T(n − 1) + T(n − 2) + 3 for n > 1

We see that T(n) ≥ F<sub>n</sub>. This implies that the running time of the algorithm grows as fast as the
Fibonacci numbers! T(n) is exponential in n, which implies that the algorithm is
impractically slow except for very small values of n.
So this naive algorithm is correct but inefficient in time complexity.

### Algorithm 2 to calculate F <sub>n</sub>

fib1 was inefficient because many computations are repeated in the recursive calls. 
A more sensible scheme would store the intermediate results—the values F <sub>0</sub> , F <sub>1</sub> , . . . ,F <sub>n-1</sub> as soon as they become known. 
fib2 does the same:

```python
function fib2(n)
if n = 0: return 0
create an array f[0 . . . n]
f[0] = 0, f[1] = 1
for i = 2 . . . n:
f[i] = f[i − 1] + f[i − 2]
return f[n]
```

#### Analysis:

The time complexity appears to be linear but it is not for larger values of n.
It is reasonable to treat addition as a single computer step if small numbers are being added, 32-bit numbers say. But the nth Fibonacci number is about 0.694n bits long, and this can far exceed 32 as n grows.
Given that the addition of two n-bit numbers takes time roughly proportional to n, the time complexity of fib2 is proportional to n<sup>2</sup>.

### Algorithm 3 to calculate F <sub>n</sub>

We use matrix multiplication to calculate F <sub>n</sub>. We express each F <sub>i</sub> in the form of a 2*2 matrix.
As in,

![fib_matrix_0](fib_matrix_0.png)

![fib_matrix_n](fib_matrix_n.png)

#### Analysis:

We observe that we can reach F <sub>n</sub> by multiplying the initial matrix by 2 for log(n) times. Also, multiplication involves n-bit integers. Therefore, if we assume the time complexity of multiplication is M(n), the total time complexity of fib3 is O(M(n)*log(n)).

### Algorithm 4 to calculate F <sub>n</sub>

We can use the direct formula for F <sub>n</sub>:

![fib_formula](fib_formula.png)

#### Analysis:

We observe that we can reach F <sub>n</sub> by multiplying the irrational number by 2 for log(n) times. Also, multiplication involves n-bit irrationals. Therefore, if we assume the time complexity of multiplication is M(n), the total time complexity of fib3 is O(M(n)*log(n)). But this method is not preferred because multiplication of irrationals to higher powers might cause accuracy deficits.

## Karatsuba algorithm for multiplication of two large numbers

We use divide and conquer approach. Suppose x and y are two n-bit integers. As a first step
toward multiplying x and y, split each of them into their left and right halves, which are n/2 bits long:

x = 2<sup>n/2</sup>x<sub>L</sub> + x<sub>R</sub>
y = 2<sup>n/2</sup>y<sub>L</sub> + y<sub>R</sub>

xy = (2<sup>n/2</sup>x<sub>L</sub> + x<sub>R</sub>)(2<sup>n/2</sup>y<sub>L</sub> + y<sub>R</sub>)
   = 2<sup>n</sup>x<sub>L</sub>y<sub>L</sub> + 2<sup>n/2</sup>((x<sub>L</sub> + x<sub>R</sub>)(y<sub>L</sub> + y<sub>R</sub>) - x<sub>L</sub>y<sub>L</sub> - x<sub>R</sub>y<sub>R</sub>) + x<sub>R</sub>y<sub>R</sub>

```python
function multiply(x, y)
Input: n-bit positive integers x and y
Output: Their product
if n = 1: return xy
x L , x R = leftmost ceil(n/2), rightmost floor(n/2) bits of x
y L , y R = leftmost ceil(n/2), rightmost floor(n/2) bits of y
P 1 = multiply(x L , y L )
P 2 = multiply(x R , y R )
P 3 = multiply(x L + x R , y L + y R )
return P 1 × 2^(n) + (P 3 − P 1 − P 2 ) × 2^(n/2) + P 2
```

#### Analysis:

The recurrence relation for the above algorithm is:

T(n) = 3T(n/2) + O(n)

The total time complexity comes out to be O(n<sup>1.59</sup>).


# Week 2, Lecture 2

## Recurrence Relations (Master Theorem)

Divide-and-conquer algorithms often follow a generic pattern: they tackle a problem of size n by recursively solving, say, a subproblems of size n/b and then combining these answers in O(n<sup>d</sup>) time, for some a, b, d > 0 
(in the multiplication algorithm, a = 3, b = 2, and d = 1).
Therefore, their running time comes out to be:

T(n) = aT(ceil(n/b)) + O(n<sup>d</sup>)

The Master Theorem states that time complexity of the above recurrence relation is:

T(n) = O(n<sup>d</sup>)                 if d > log<sub>b</sub>a

T(n) = O(n<sup>d</sup>log(n))           if d = log<sub>b</sub>a

T(n) = O(n<sup>log<sub>b</sub>a</sup>)  if d < log<sub>b</sub>a

#### Analysis:

![master_theorem_tree](master_theorem_tree.png)

Notice that the size of the subproblems decreases by a factor of b with each level of recursion, and therefore reaches the base case after log<sub>b</sub>n levels. This is the height of the recursion tree. Its branching factor is a, so the kth level of the tree is made up of a k subproblems, each of size n/b<sup>k</sup>. The total work done at this level is:

a<sup>k</sup> * O(n/b<sup>k</sup>)<sup>d</sup> = O(n<sup>d</sup>) * (a/b<sup>d</sup>)<sup>k</sup>

As k goes from 0 (the root) to log<sub>b</sub>n (the leaves), these numbers form a geometric series with ratio a/b<sup>d</sup>. After we find the sum of this series, we get three cases:

1. The ratio is less than 1.
   Then the series is decreasing, and its sum is just given by its first term, O(n<sup>d</sup>). 
2. The ratio is greater than 1.
   The series is increasing and its sum is given by its last term, O(n<sup>log<sub>b</sub>a</sup>):

   n<sup>d</sup> * (a/b<sup>d</sup>)<sup>log<sub>b</sub>n</sup> =  n<sup>d</sup> * (a<sup>log<sub>b</sub>n</sup>)/(b<sup>log<sub>b</sub>n</sup>)<sup>d</sup> = a<sup>log<sub>b</sub>n</sup> = a<sup>(log<sub>a</sub>n)(log<sub>b</sub>a)</sup> = n<sup>log<sub>b</sub>a</sup>
3. The ratio is exactly 1.
   In this case all O(log(n)) terms of the series are equal to O(n<sup>d</sup>).   

These cases translate directly into the three contingencies in the Master Theorem.

### Binary Search

The ultimate divide-and-conquer algorithm is, of course, binary search: to find a key k in a large file containing keys z[0, 1, . . . , n − 1] in sorted order, we first compare k with z[n/2], and depending on the result we recurse either on the first half of the file, z[0, . . . , n/2 − 1], or on the second half, z[n/2, . . . , n − 1]. The recurrence now is T(n) = T(ceil(n/2)) + O(1), which is the case a = 1, b = 2, d = 0. Plugging into our master theorem we get the familiar solution: a running time of just O(log(n)).

## Merge Sort

The problem of sorting a list of numbers lends itself immediately to a divide-and-
conquer strategy: split the list into two halves, recursively sort each half, and then
merge the two sorted sublists.

```python
function mergesort(a[1 . . . n])
Input: An array of numbers a[1 . . . n]
Output: A sorted version of this array
if n > 1:
return merge(mergesort(a[1 . . . floor(n/2)]),
mergesort(a[floor(n/2) + 1 . . . n]))
else:
return a
```

```python
function merge(x[1 . . . k], y[1 . . . l])
if k = 0: return y[1 . . . l]
if l = 0: return x[1 . . . k]
if x[1] ≤ y[1]:
return x[1] ◦ merge(x[2 . . . k], y[1 . . . l])
else:
return y[1] ◦ merge(x[1 . . . k], y[2 . . . l])
```

Here ◦ denotes concatenation. This merge procedure does a constant amount of work per recursive call (provided the required array space is allocated in advance), for a total running time of O(k + l). Thus merge’s are linear, and the recurrence relation of this mergesort is:

T(n) = 2T(n/2) + O(n)

whose time complexity comes out to be O(nlog(n)). (From Master theorem)

### Iterative merge sort:

At any given moment, there is a set of “active” arrays; initially, the singletons; which are merged in pairs to give the next batch of active arrays. These arrays can be organized in a queue, and processed by repeatedly removing two arrays from the front of the queue, merging them, and putting the result at the end of the queue.

```python
function iterative-mergesort(a[1 . . . n])
Input: elements a 1 , a 2 , . . . , a n to be sorted
Q = [ ] (empty queue)
for i = 1 to n:
inject(Q, [a i ])
while |Q| > 1:
inject(Q, merge(eject(Q), eject(Q)))
return eject(Q)
```

The inject operation adds an element to the end of the queue while eject removes and returns the element at the front of the queue.

## Matrix multiplication

Consider two (n × n) matrices X and Y.
Matrix multiplication is particularly easy to break into subproblems, because it can be performed blockwise, i.e carve X into four n/2 × n/2 blocks, and also Y:

![simple_matrix](simple_matrix.png)

Now, their product can be expressed in terms of these blocks and is exactly as if the blocks were single elements:

![simple_matrix_product](simple_matrix_product.png)

We now have a divide-and-conquer strategy: to compute the size-n product XY, recursively compute eight size-n/2 products AE , B G , AF , B H, C E , DG , C F , D H , and then do a few O(n 2 )-time additions. The total running time is described by the recurrence relation:

T(n) = 8T(n/2) + O(n<sup>2</sup>)

But this comes out to be O(n<sup>3</sup>) which is inefficient. 
There's a better way to compute XY, XY can be computed from just seven n/2 × n/2 subproblems by a tricky decomposition - **Strassen's algorithm**.

![strassen](strassen.png)

where,

P<sub>1</sub> = A(F − H)          

P<sub>2</sub> = (A + B)H             

P<sub>3</sub> = (C + D)E             

P<sub>4</sub> = D(G − E)

P<sub>5</sub> = (A + D)(E + H)

P<sub>6</sub> = (B − D)(G + H)

P<sub>7</sub> = (A − C)(E + F)

The new running time is:

T(n) = 7T(n/2) + O(n<sup>2</sup>),

which by the master theorem comes out to be O(n<sup>log<sub>2</sub>7</sup>) ~ O(n<sup>2.81</sup>)

## Median

Using merge sort, we can find the median of a given list of numbers in O(nlog(n)).

But there exists a better approach whose time complexity is O(n).
For any number v, imagine splitting list S into three categories: elements smaller than v, those equal to v (there
might be duplicates), and those greater than v. Call these S<sub>L</sub>, S<sub>v</sub> and S<sub>R</sub> respectively.

selection(S, k) = selection(S<sub>L</sub>, k)                   if k ≤ |S<sub>L</sub>|
selection(S, k) = v                                    if |S<sub>L</sub>| < k ≤ |S<sub>L</sub>| + |S<sub>v</sub>|
selection(S, k) = selection(S<sub>R</sub>, k − |S<sub>L</sub>| − |S<sub>v</sub>|) if k > |S<sub>L</sub>| + |S<sub>v</sub>|.

The effect of the split is to shrink the number of elements from |S| to at most max{|S|<sub>L</sub>, |S<sub>R</sub>|}.
We now pick v randomly from S.
The running time of our algorithm depends on the random choices of v. It is possible that due to persistent bad luck we keep picking v to be the largest element of the array (or the smallest element), and thereby shrink the array by only one element each time. This worst-case scenario would force our selection algorithm to
perform O(n<sup>2</sup>) operations, but it is extremely unlikely to occur. Equally unlikely is the best possible case in which each randomly chosen v just happens to split the array perfectly in half, resulting in a running
time of O(n).

To distinguish between lucky and unlucky choices of v, we will call v good if it lies within the 25th to 75th percentile of the array that it is chosen from. We like these choices of v because they ensure that the sublists S<sub>L</sub> and S<sub>R</sub> have size at most three-fourths that of S, so that the array shrinks substantially.

Therefore, after two split operations on average, the array will shrink to at most three-fourths of its size. Letting T(n) be the expected running time on an array of size n, we get:

T(n) ≤ T(3n/4) + O(n)

From this recurrence relation, we get the time complexity to be O(n). (from master theorem)